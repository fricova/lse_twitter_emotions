{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fricova/lse_twitter_emotions/blob/main/main_analysis_FCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT3BwGcQ8kfU"
      },
      "source": [
        "import keras # define and train NN\n",
        "import sklearn # train test split\n",
        "import pandas as pd\n",
        "import numpy as np # matrix multiplication\n",
        "from sklearn.preprocessing import MinMaxScaler # MinMaxScaler to scale everything [0,1]\n",
        "import matplotlib.pyplot as plt # plotting\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aP3q5S09Bpg",
        "outputId": "7d6fbf25-bdbc-4814-dad3-8592c0352359"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8eelZIHwCwy",
        "outputId": "a7613ada-b694-4606-ab26-50a28fc2cdbf"
      },
      "source": [
        "!ls /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'\t\t data_for_training_without_weekend.csv\n",
            "'Copy of coronatweets.ipynb'\t'Glemping Trip'\n",
            " corona_tweets_101-200.zip\t'PB434 Seminar Prezi.gslides'\n",
            " corona_tweets_1-100.zip\t PRICES_NO_WEEKENDS.csv\n",
            " corona_tweets_theta-omega.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "4QeZxWMc_W8F",
        "outputId": "52343062-0bc9-470c-8a94-42b06dd62d65"
      },
      "source": [
        "data = pd.read_csv(\"/content/gdrive/MyDrive/PRICES_NO_WEEKENDS.csv\")\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>anticipation</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>trust</th>\n",
              "      <th>syuzhet_index</th>\n",
              "      <th>bing_index</th>\n",
              "      <th>nrc_index</th>\n",
              "      <th>bing_index_norm</th>\n",
              "      <th>nrc_index_norm</th>\n",
              "      <th>syuzhet_index_norm</th>\n",
              "      <th>SP500</th>\n",
              "      <th>fear_norm</th>\n",
              "      <th>joy_norm</th>\n",
              "      <th>trust_norm</th>\n",
              "      <th>anticipation_norm</th>\n",
              "      <th>syuzhet_index_z</th>\n",
              "      <th>fear_z</th>\n",
              "      <th>joy_z</th>\n",
              "      <th>anticipation_z</th>\n",
              "      <th>trust_z</th>\n",
              "      <th>nrc_index_z</th>\n",
              "      <th>bing_index_z</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2020-02-28</td>\n",
              "      <td>0.485801</td>\n",
              "      <td>0.703907</td>\n",
              "      <td>0.281224</td>\n",
              "      <td>0.602122</td>\n",
              "      <td>-0.425539</td>\n",
              "      <td>-0.599669</td>\n",
              "      <td>-0.346609</td>\n",
              "      <td>-0.800205</td>\n",
              "      <td>-0.651246</td>\n",
              "      <td>-0.695493</td>\n",
              "      <td>2954.22</td>\n",
              "      <td>0.793227</td>\n",
              "      <td>-0.113220</td>\n",
              "      <td>-0.037947</td>\n",
              "      <td>0.634220</td>\n",
              "      <td>-3.174923</td>\n",
              "      <td>2.672333</td>\n",
              "      <td>0.430044</td>\n",
              "      <td>1.928683</td>\n",
              "      <td>0.465933</td>\n",
              "      <td>-2.886255</td>\n",
              "      <td>-3.043472</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2020-03-02</td>\n",
              "      <td>0.436957</td>\n",
              "      <td>0.617359</td>\n",
              "      <td>0.221443</td>\n",
              "      <td>0.632146</td>\n",
              "      <td>-0.324243</td>\n",
              "      <td>-0.498966</td>\n",
              "      <td>-0.147359</td>\n",
              "      <td>-0.566565</td>\n",
              "      <td>-0.106651</td>\n",
              "      <td>-0.370656</td>\n",
              "      <td>3090.23</td>\n",
              "      <td>0.328403</td>\n",
              "      <td>-0.886903</td>\n",
              "      <td>0.077953</td>\n",
              "      <td>0.282165</td>\n",
              "      <td>-2.175104</td>\n",
              "      <td>1.275395</td>\n",
              "      <td>-1.976391</td>\n",
              "      <td>0.492236</td>\n",
              "      <td>1.114476</td>\n",
              "      <td>-1.329017</td>\n",
              "      <td>-2.315678</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>2020-03-03</td>\n",
              "      <td>0.473808</td>\n",
              "      <td>0.674835</td>\n",
              "      <td>0.247063</td>\n",
              "      <td>0.583301</td>\n",
              "      <td>-0.336651</td>\n",
              "      <td>-0.503119</td>\n",
              "      <td>-0.244285</td>\n",
              "      <td>-0.576201</td>\n",
              "      <td>-0.371573</td>\n",
              "      <td>-0.410445</td>\n",
              "      <td>3003.37</td>\n",
              "      <td>0.637090</td>\n",
              "      <td>-0.555337</td>\n",
              "      <td>-0.110602</td>\n",
              "      <td>0.547775</td>\n",
              "      <td>-2.297570</td>\n",
              "      <td>2.203092</td>\n",
              "      <td>-0.945099</td>\n",
              "      <td>1.575972</td>\n",
              "      <td>0.059378</td>\n",
              "      <td>-2.086544</td>\n",
              "      <td>-2.345693</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>2020-03-04</td>\n",
              "      <td>0.475025</td>\n",
              "      <td>0.643650</td>\n",
              "      <td>0.251432</td>\n",
              "      <td>0.569121</td>\n",
              "      <td>-0.314615</td>\n",
              "      <td>-0.510885</td>\n",
              "      <td>-0.229937</td>\n",
              "      <td>-0.594217</td>\n",
              "      <td>-0.332357</td>\n",
              "      <td>-0.339779</td>\n",
              "      <td>3130.12</td>\n",
              "      <td>0.469602</td>\n",
              "      <td>-0.498784</td>\n",
              "      <td>-0.165340</td>\n",
              "      <td>0.556547</td>\n",
              "      <td>-2.080067</td>\n",
              "      <td>1.699740</td>\n",
              "      <td>-0.769200</td>\n",
              "      <td>1.611764</td>\n",
              "      <td>-0.246916</td>\n",
              "      <td>-1.974410</td>\n",
              "      <td>-2.401815</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>2020-03-05</td>\n",
              "      <td>0.479278</td>\n",
              "      <td>0.649734</td>\n",
              "      <td>0.248655</td>\n",
              "      <td>0.575449</td>\n",
              "      <td>-0.319109</td>\n",
              "      <td>-0.512931</td>\n",
              "      <td>-0.265848</td>\n",
              "      <td>-0.598964</td>\n",
              "      <td>-0.430508</td>\n",
              "      <td>-0.354191</td>\n",
              "      <td>3023.94</td>\n",
              "      <td>0.502277</td>\n",
              "      <td>-0.534732</td>\n",
              "      <td>-0.140911</td>\n",
              "      <td>0.587202</td>\n",
              "      <td>-2.124428</td>\n",
              "      <td>1.797937</td>\n",
              "      <td>-0.881010</td>\n",
              "      <td>1.736844</td>\n",
              "      <td>-0.110221</td>\n",
              "      <td>-2.255066</td>\n",
              "      <td>-2.416603</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>216</td>\n",
              "      <td>2020-09-29</td>\n",
              "      <td>0.442886</td>\n",
              "      <td>0.561427</td>\n",
              "      <td>0.270502</td>\n",
              "      <td>0.586078</td>\n",
              "      <td>-0.071379</td>\n",
              "      <td>-0.090919</td>\n",
              "      <td>0.075395</td>\n",
              "      <td>0.380138</td>\n",
              "      <td>0.502184</td>\n",
              "      <td>0.440235</td>\n",
              "      <td>3335.47</td>\n",
              "      <td>0.028009</td>\n",
              "      <td>-0.251989</td>\n",
              "      <td>-0.099882</td>\n",
              "      <td>0.324902</td>\n",
              "      <td>0.320745</td>\n",
              "      <td>0.372619</td>\n",
              "      <td>-0.001579</td>\n",
              "      <td>0.666608</td>\n",
              "      <td>0.119364</td>\n",
              "      <td>0.411912</td>\n",
              "      <td>0.633319</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>217</td>\n",
              "      <td>2020-09-30</td>\n",
              "      <td>0.423321</td>\n",
              "      <td>0.565903</td>\n",
              "      <td>0.272983</td>\n",
              "      <td>0.610541</td>\n",
              "      <td>-0.095622</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>0.099206</td>\n",
              "      <td>0.593766</td>\n",
              "      <td>0.567266</td>\n",
              "      <td>0.362491</td>\n",
              "      <td>3363.00</td>\n",
              "      <td>0.052049</td>\n",
              "      <td>-0.219881</td>\n",
              "      <td>-0.005446</td>\n",
              "      <td>0.183885</td>\n",
              "      <td>0.081457</td>\n",
              "      <td>0.444866</td>\n",
              "      <td>0.098289</td>\n",
              "      <td>0.091234</td>\n",
              "      <td>0.647802</td>\n",
              "      <td>0.598008</td>\n",
              "      <td>1.298775</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>218</td>\n",
              "      <td>2020-10-01</td>\n",
              "      <td>0.438175</td>\n",
              "      <td>0.539686</td>\n",
              "      <td>0.298725</td>\n",
              "      <td>0.589078</td>\n",
              "      <td>-0.003546</td>\n",
              "      <td>-0.058103</td>\n",
              "      <td>0.145464</td>\n",
              "      <td>0.456275</td>\n",
              "      <td>0.693699</td>\n",
              "      <td>0.657762</td>\n",
              "      <td>3380.80</td>\n",
              "      <td>-0.088755</td>\n",
              "      <td>0.113271</td>\n",
              "      <td>-0.088301</td>\n",
              "      <td>0.290947</td>\n",
              "      <td>0.990272</td>\n",
              "      <td>0.021706</td>\n",
              "      <td>1.134513</td>\n",
              "      <td>0.528069</td>\n",
              "      <td>0.184171</td>\n",
              "      <td>0.959536</td>\n",
              "      <td>0.870487</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>219</td>\n",
              "      <td>2020-10-02</td>\n",
              "      <td>0.453991</td>\n",
              "      <td>0.447291</td>\n",
              "      <td>0.322588</td>\n",
              "      <td>0.680911</td>\n",
              "      <td>-0.006623</td>\n",
              "      <td>0.176253</td>\n",
              "      <td>0.156574</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.724066</td>\n",
              "      <td>0.647895</td>\n",
              "      <td>3348.44</td>\n",
              "      <td>-0.584981</td>\n",
              "      <td>0.422097</td>\n",
              "      <td>0.266203</td>\n",
              "      <td>0.404939</td>\n",
              "      <td>0.959904</td>\n",
              "      <td>-1.469604</td>\n",
              "      <td>2.095075</td>\n",
              "      <td>0.993175</td>\n",
              "      <td>2.167869</td>\n",
              "      <td>1.046368</td>\n",
              "      <td>2.564202</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>222</td>\n",
              "      <td>2020-10-05</td>\n",
              "      <td>0.485202</td>\n",
              "      <td>0.585882</td>\n",
              "      <td>0.263027</td>\n",
              "      <td>0.871000</td>\n",
              "      <td>-0.247731</td>\n",
              "      <td>-0.071934</td>\n",
              "      <td>0.010762</td>\n",
              "      <td>0.424185</td>\n",
              "      <td>0.325528</td>\n",
              "      <td>-0.125294</td>\n",
              "      <td>3408.63</td>\n",
              "      <td>0.159350</td>\n",
              "      <td>-0.348722</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.629902</td>\n",
              "      <td>-1.419903</td>\n",
              "      <td>0.767339</td>\n",
              "      <td>-0.302452</td>\n",
              "      <td>1.911068</td>\n",
              "      <td>6.273979</td>\n",
              "      <td>-0.093227</td>\n",
              "      <td>0.770527</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0        date  anticipation  ...  nrc_index_z  bing_index_z  weekday\n",
              "0             2  2020-02-28      0.485801  ...    -2.886255     -3.043472        5\n",
              "1             5  2020-03-02      0.436957  ...    -1.329017     -2.315678        1\n",
              "2             6  2020-03-03      0.473808  ...    -2.086544     -2.345693        2\n",
              "3             7  2020-03-04      0.475025  ...    -1.974410     -2.401815        3\n",
              "4             8  2020-03-05      0.479278  ...    -2.255066     -2.416603        4\n",
              "..          ...         ...           ...  ...          ...           ...      ...\n",
              "148         216  2020-09-29      0.442886  ...     0.411912      0.633319        2\n",
              "149         217  2020-09-30      0.423321  ...     0.598008      1.298775        3\n",
              "150         218  2020-10-01      0.438175  ...     0.959536      0.870487        4\n",
              "151         219  2020-10-02      0.453991  ...     1.046368      2.564202        5\n",
              "152         222  2020-10-05      0.485202  ...    -0.093227      0.770527        1\n",
              "\n",
              "[153 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28enWBvuXwqR"
      },
      "source": [
        "from keras import layers # modules \n",
        "from keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT5Uzyi9N4Rv"
      },
      "source": [
        "sp500 = data['SP500']\n",
        "sp500_mean = np.mean(sp500.values)\n",
        "sp500_std = np.std(sp500.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX1OFwxhoFgW"
      },
      "source": [
        "def direction_accuracy(x, y, model):\n",
        "  real_difference = y - x[:, -1]\n",
        "  real_direction = np.sign(real_difference)\n",
        "  predicted_difference = model.predict(x) - x[:, -1]\n",
        "  predicted_direction = np.sign(predicted_difference)\n",
        "  return (real_direction == predicted_direction).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTLRZBgkeEnc"
      },
      "source": [
        "def create_model(n_of_inputs):\n",
        "  input = keras.Input(shape=(n_of_inputs))\n",
        "  x = layers.Dense(32)(input) # the first layer is dense/fully connected, not sparse - normal NN layer\n",
        "  x = layers.LeakyReLU()(x)  # the first activation function is a leaky ReLu, because just 0 would make the NN quite difficult to train\n",
        "  x = layers.Dense(16)(x) # the second layer\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  x = layers.Dense(1)(x) # the last layer outputs just one thing \n",
        "\n",
        "  model = keras.Model(inputs=input, outputs=x)\n",
        "  # model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_percentage_error']) # we use adam optimizer, this means how you change the weights given the derivative w.r.t. the weight\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o-UFpsBcbN4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def get_data_for_columns(d, columns, lag, k, k_index):\n",
        "  d = d.copy()\n",
        "  length_of_1_k = len(d) // k\n",
        "  d_train_first = d[0:length_of_1_k * k_index]\n",
        "  d_test = d[length_of_1_k * k_index:(length_of_1_k * k_index) + length_of_1_k]\n",
        "  d_train_second = d[(length_of_1_k * k_index) + length_of_1_k:-1]\n",
        "  if len(columns) > 0:\n",
        "    d[columns] = MinMaxScaler().fit_transform(d[columns]) # normalizing [0,1]\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  X_test = []\n",
        "  y_test = []\n",
        "  for i in range(0, len(d_train_first) - lag - 1):\n",
        "    emotions = d_train_first[i:i+(lag)][columns] # 3 columns for each emotion\n",
        "    sp500 = d_train_first[i:i+(lag+1)]['SP500'].values\n",
        "    # sp500 = (sp500 - sp500_mean) / sp500_std\n",
        "    # sp500_differences = np.diff(sp500.values)\n",
        "    X_train.append(np.concatenate((emotions.values.flatten(), sp500[:-1]))) # we join all emotion lists together + SP500 as last columns \n",
        "    y_train.append(sp500[-1]) # we are predicting last value of SP500\n",
        "  for i in range(0, len(d_train_second) - lag - 1):\n",
        "    emotions = d_train_second[i:i+(lag)][columns] # 3 columns for each emotion\n",
        "    sp500 = d_train_second[i:i+(lag+1)]['SP500'].values\n",
        "    # sp500 = (sp500 - sp500_mean) / sp500_std\n",
        "    # sp500_differences = np.diff(sp500.values)\n",
        "    X_train.append(np.concatenate((emotions.values.flatten(), sp500[:-1])))\n",
        "    y_train.append(sp500[-1]) # we are predicting last value of SP500\n",
        "  for i in range(0, len(d_test) - lag - 1):\n",
        "    emotions = d_test[i:i+(lag)][columns]\n",
        "    sp500 = d_test[i:i+(lag+1)]['SP500'].values # we do the same for test set\n",
        "    # sp500 = (sp500 - sp500_mean) / sp500_std\n",
        "    # sp500_differences = np.diff(sp500.values)\n",
        "\n",
        "    X_test.append(np.concatenate((emotions.values.flatten(), sp500[:-1])))\n",
        "\n",
        "    y_test.append(sp500[-1])\n",
        "  X_train = np.vstack(X_train) # we stack all training vectors together\n",
        "  y_train = np.array(y_train)\n",
        "  X_test = np.vstack(X_test) # we stack all test vectors together\n",
        "  y_test = np.array(y_test)\n",
        "  return X_train, X_test, y_train, y_test \n",
        "\n",
        "def train_model_with_columns(d, columns, lag, n_of_trials):\n",
        "  mapes = []\n",
        "  predicteds = []\n",
        "  actuals = []\n",
        "  dir_accs = []\n",
        "  for i in range(n_of_trials):\n",
        "    X_train, X_test, y_train, y_test = get_data_for_columns(d, columns, lag, n_of_trials, i)\n",
        "    model = create_model(X_train.shape[1])\n",
        "    model.fit(X_train, y_train, batch_size=10, epochs=10, verbose=0)\n",
        "    mapes.append(model.evaluate(X_test, y_test)[1])\n",
        "    predicteds.append(model.predict(X_test).flatten())\n",
        "    actuals.append(y_test)\n",
        "    dir_accs.append(direction_accuracy(X_test, y_test, model))\n",
        "  return mapes, predicteds, actuals, dir_accs\n",
        "  # we train the same model multiple times, average mape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiOUrUvsi3ww"
      },
      "source": [
        "k = 10 # the number of folds in the k-fold cross-validation is 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uq2AnfUxRXK",
        "outputId": "256d7129-e3f6-4e86-f13e-229951edaff4"
      },
      "source": [
        "mape_without_columns = 100000000\n",
        "best_lag = -1\n",
        "predictes_without_columns = []\n",
        "actuals_without_columns = []\n",
        "direction_accs_without_columns = []\n",
        "for lag in range(1, 7):\n",
        "  columns_of_interest = [] # no emotion columns in the model, just SP500 to see if adding emotions improves forecasting accuracy\n",
        "\n",
        "  curr_mapes_without_columns, curr_predictes_without_columns, curr_actuals_without_columns, curr_accs = \\\n",
        "    train_model_with_columns(data, columns_of_interest, lag, k)\n",
        "  current_mape = np.array(curr_mapes_without_columns).mean()\n",
        "  if current_mape < mape_without_columns:\n",
        "    mape_without_columns = current_mape\n",
        "    best_lag = lag\n",
        "    predictes_without_columns = curr_predictes_without_columns\n",
        "    actuals_without_columns = curr_actuals_without_columns\n",
        "    direction_accs_without_columns = curr_accs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 251ms/step - loss: 32505.6973 - mean_absolute_percentage_error: 6.1321\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 11688.5410 - mean_absolute_percentage_error: 3.5111\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 3215.9558 - mean_absolute_percentage_error: 1.7449\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 1787.9401 - mean_absolute_percentage_error: 1.2469\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 3903.9214 - mean_absolute_percentage_error: 1.3197\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 1630.6487 - mean_absolute_percentage_error: 1.0064\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 810.3514 - mean_absolute_percentage_error: 0.7873\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 343.3326 - mean_absolute_percentage_error: 0.4479\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 2547.8877 - mean_absolute_percentage_error: 1.0176\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 1490.1510 - mean_absolute_percentage_error: 0.9853\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 23672.2246 - mean_absolute_percentage_error: 4.5747\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 12825.3945 - mean_absolute_percentage_error: 3.5415\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 2614.1328 - mean_absolute_percentage_error: 1.3248\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 1953.5035 - mean_absolute_percentage_error: 1.1845\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 5124.0493 - mean_absolute_percentage_error: 1.6202\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 1672.5162 - mean_absolute_percentage_error: 1.1053\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 678.5262 - mean_absolute_percentage_error: 0.6869\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 1060.3727 - mean_absolute_percentage_error: 0.8928\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 2786.7891 - mean_absolute_percentage_error: 1.2828\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 2293.4260 - mean_absolute_percentage_error: 1.2766\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 30629.4766 - mean_absolute_percentage_error: 5.2782\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 26454.6191 - mean_absolute_percentage_error: 5.3480\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 2380.5515 - mean_absolute_percentage_error: 1.2792\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 2580.6611 - mean_absolute_percentage_error: 1.4130\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 6663.9814 - mean_absolute_percentage_error: 1.9254\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 1868.9847 - mean_absolute_percentage_error: 1.1255\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 675.6589 - mean_absolute_percentage_error: 0.7188\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 999.8131 - mean_absolute_percentage_error: 0.8483\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 6860.3599 - mean_absolute_percentage_error: 1.9902\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 2694.0466 - mean_absolute_percentage_error: 1.3629\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 41801.6641 - mean_absolute_percentage_error: 6.6069\n",
            "1/1 [==============================] - 0s 277ms/step - loss: 17217.0723 - mean_absolute_percentage_error: 4.4678\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 2726.4927 - mean_absolute_percentage_error: 1.3820\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 2830.6062 - mean_absolute_percentage_error: 1.4980\n",
            "1/1 [==============================] - 0s 243ms/step - loss: 9962.7051 - mean_absolute_percentage_error: 2.5511\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 2571.0469 - mean_absolute_percentage_error: 1.3420\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 862.3806 - mean_absolute_percentage_error: 0.8572\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 340.8643 - mean_absolute_percentage_error: 0.4610\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 7573.3877 - mean_absolute_percentage_error: 2.1828\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 5081.0938 - mean_absolute_percentage_error: 1.8443\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 33123.4062 - mean_absolute_percentage_error: 5.6379\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 13179.1807 - mean_absolute_percentage_error: 3.6818\n",
            "1/1 [==============================] - 0s 281ms/step - loss: 4704.2412 - mean_absolute_percentage_error: 1.9681\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 3643.7974 - mean_absolute_percentage_error: 1.8886\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 9319.1768 - mean_absolute_percentage_error: 2.5497\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 3210.7488 - mean_absolute_percentage_error: 1.5037\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 1467.4329 - mean_absolute_percentage_error: 1.0981\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 1333.6748 - mean_absolute_percentage_error: 0.9075\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 6407.8213 - mean_absolute_percentage_error: 2.1419\n",
            "1/1 [==============================] - 0s 245ms/step - loss: 3212.6262 - mean_absolute_percentage_error: 1.5194\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 49715.8906 - mean_absolute_percentage_error: 7.2463\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 14971.2080 - mean_absolute_percentage_error: 4.2961\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 3642.9902 - mean_absolute_percentage_error: 1.6961\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 2407.1680 - mean_absolute_percentage_error: 1.3461\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 9818.6211 - mean_absolute_percentage_error: 2.8545\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 3680.9004 - mean_absolute_percentage_error: 1.9125\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 999.6400 - mean_absolute_percentage_error: 0.8727\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1444.6589 - mean_absolute_percentage_error: 0.9781\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 7292.0928 - mean_absolute_percentage_error: 2.1219\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 3387.4097 - mean_absolute_percentage_error: 1.5425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JgEzpF8znD9",
        "outputId": "afeb6c2b-1554-4a27-cc42-d86c21a3145b"
      },
      "source": [
        "inputs = [\n",
        "  {\"columns\": [\"fear\"], \"name\": \"fear\" },\n",
        "  {\"columns\": [\"syuzhet_index\"], \"name\": \"index\"},\n",
        "  {\"columns\": [\"fear\", \"anticipation\"], \"name\": \"fear_anticipation\" },\n",
        "  {\"columns\": [\"fear\", \"joy\"], \"name\": \"fear_joy\" },\n",
        "  {\"columns\": [\"fear\", \"trust\"], \"name\": \"fear_trust\" },\n",
        "  {\"columns\": [\"fear\", \"joy\", \"anticipation\"], \"name\": \"fear_joy_anticipation\" },\n",
        "  {\"columns\": [\"fear\", \"joy\", \"trust\"], \"name\": \"fear_joy_trust\" },\n",
        "  {\"columns\": [\"fear\", \"joy\", \"anticipation\",\"trust\"], \"name\": \"fear_joy_anticipation_trust\" },\n",
        "]\n",
        "\n",
        "outputs = {}\n",
        "\n",
        "for input in inputs:\n",
        "  columns_of_interest = input[\"columns\"]\n",
        "  mapes, predictes, actuals, accs = train_model_with_columns(data, columns_of_interest, best_lag, k)\n",
        "  mape = np.array(mapes).mean()\n",
        "  outputs[input[\"name\"]] = {\"mape\": mape, \"predicteds\": predictes, \"actuals\": actuals, \"dir_accs\": accs}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 296ms/step - loss: 23335.4219 - mean_absolute_percentage_error: 4.2691\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 10974.9912 - mean_absolute_percentage_error: 3.2384\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 3061.9756 - mean_absolute_percentage_error: 1.4686\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 2143.5886 - mean_absolute_percentage_error: 1.1971\n",
            "1/1 [==============================] - 0s 286ms/step - loss: 7128.8267 - mean_absolute_percentage_error: 2.0848\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 2124.0825 - mean_absolute_percentage_error: 1.2870\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 675.9839 - mean_absolute_percentage_error: 0.6615\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 710.0665 - mean_absolute_percentage_error: 0.7198\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 3850.0642 - mean_absolute_percentage_error: 1.2559\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 1729.3579 - mean_absolute_percentage_error: 1.0994\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 22886.7656 - mean_absolute_percentage_error: 4.3170\n",
            "1/1 [==============================] - 0s 266ms/step - loss: 10904.4062 - mean_absolute_percentage_error: 3.3964\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 4008.4309 - mean_absolute_percentage_error: 1.8968\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 2112.7266 - mean_absolute_percentage_error: 1.1848\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 5091.3433 - mean_absolute_percentage_error: 1.5796\n",
            "1/1 [==============================] - 0s 257ms/step - loss: 1639.0748 - mean_absolute_percentage_error: 1.0404\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 730.1240 - mean_absolute_percentage_error: 0.6979\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 343.9873 - mean_absolute_percentage_error: 0.4690\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 2717.7612 - mean_absolute_percentage_error: 1.1387\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 2072.3796 - mean_absolute_percentage_error: 1.1993\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 23642.2969 - mean_absolute_percentage_error: 4.6547\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 10618.8750 - mean_absolute_percentage_error: 3.2144\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 4311.1460 - mean_absolute_percentage_error: 1.9909\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 1990.3901 - mean_absolute_percentage_error: 1.1907\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 4876.5352 - mean_absolute_percentage_error: 1.4030\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 1955.5297 - mean_absolute_percentage_error: 1.2088\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 607.0664 - mean_absolute_percentage_error: 0.6492\n",
            "1/1 [==============================] - 0s 251ms/step - loss: 548.3935 - mean_absolute_percentage_error: 0.6229\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 3942.0059 - mean_absolute_percentage_error: 1.3713\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 2553.9290 - mean_absolute_percentage_error: 1.3468\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 22218.9531 - mean_absolute_percentage_error: 4.0684\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 11682.9561 - mean_absolute_percentage_error: 3.3068\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 3167.4534 - mean_absolute_percentage_error: 1.5655\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 2220.5627 - mean_absolute_percentage_error: 1.2338\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 4260.2437 - mean_absolute_percentage_error: 1.2673\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 1988.2965 - mean_absolute_percentage_error: 1.1496\n",
            "1/1 [==============================] - 0s 264ms/step - loss: 693.8584 - mean_absolute_percentage_error: 0.6692\n",
            "1/1 [==============================] - 0s 246ms/step - loss: 1136.7379 - mean_absolute_percentage_error: 0.9328\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 2881.5457 - mean_absolute_percentage_error: 1.1964\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 2314.7949 - mean_absolute_percentage_error: 1.2873\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 23823.8262 - mean_absolute_percentage_error: 4.7467\n",
            "1/1 [==============================] - 1s 650ms/step - loss: 11546.7549 - mean_absolute_percentage_error: 3.3043\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 2631.0728 - mean_absolute_percentage_error: 1.3544\n",
            "1/1 [==============================] - 0s 260ms/step - loss: 2280.4651 - mean_absolute_percentage_error: 1.2873\n",
            "1/1 [==============================] - 0s 273ms/step - loss: 4674.4624 - mean_absolute_percentage_error: 1.5620\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 1685.7437 - mean_absolute_percentage_error: 1.0603\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 613.0726 - mean_absolute_percentage_error: 0.6601\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 454.0404 - mean_absolute_percentage_error: 0.5678\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 3291.2773 - mean_absolute_percentage_error: 1.2233\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 2074.3464 - mean_absolute_percentage_error: 1.2064\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 22551.0859 - mean_absolute_percentage_error: 4.0939\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 11591.4297 - mean_absolute_percentage_error: 3.3795\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 2938.7092 - mean_absolute_percentage_error: 1.4458\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 1993.1689 - mean_absolute_percentage_error: 1.1814\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 5290.1304 - mean_absolute_percentage_error: 1.6490\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 1704.9899 - mean_absolute_percentage_error: 1.0764\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 663.5781 - mean_absolute_percentage_error: 0.6569\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 811.8611 - mean_absolute_percentage_error: 0.7654\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 3914.1660 - mean_absolute_percentage_error: 1.3975\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 1836.7819 - mean_absolute_percentage_error: 1.1386\n",
            "1/1 [==============================] - 0s 272ms/step - loss: 25536.6504 - mean_absolute_percentage_error: 4.8958\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 11556.4307 - mean_absolute_percentage_error: 3.3543\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 2631.0999 - mean_absolute_percentage_error: 1.4243\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 1888.8115 - mean_absolute_percentage_error: 1.2304\n",
            "1/1 [==============================] - 0s 252ms/step - loss: 4661.7173 - mean_absolute_percentage_error: 1.5303\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 1804.4196 - mean_absolute_percentage_error: 1.1292\n",
            "1/1 [==============================] - 0s 268ms/step - loss: 727.0701 - mean_absolute_percentage_error: 0.6822\n",
            "1/1 [==============================] - 0s 262ms/step - loss: 525.1819 - mean_absolute_percentage_error: 0.6114\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 3683.9734 - mean_absolute_percentage_error: 1.3607\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 2043.4352 - mean_absolute_percentage_error: 1.2050\n",
            "1/1 [==============================] - 0s 254ms/step - loss: 23380.4922 - mean_absolute_percentage_error: 4.3756\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 11020.1982 - mean_absolute_percentage_error: 3.3216\n",
            "1/1 [==============================] - 0s 275ms/step - loss: 2947.5291 - mean_absolute_percentage_error: 1.4389\n",
            "1/1 [==============================] - 0s 249ms/step - loss: 2274.4490 - mean_absolute_percentage_error: 1.2339\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 4217.4731 - mean_absolute_percentage_error: 1.2757\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 1934.8120 - mean_absolute_percentage_error: 1.2565\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 679.0573 - mean_absolute_percentage_error: 0.6459\n",
            "1/1 [==============================] - 0s 280ms/step - loss: 514.9366 - mean_absolute_percentage_error: 0.6073\n",
            "1/1 [==============================] - 0s 258ms/step - loss: 5722.9009 - mean_absolute_percentage_error: 1.5221\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 2361.9426 - mean_absolute_percentage_error: 1.2927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "9Y_l31lRCnG-",
        "outputId": "7f73aa1c-6ea2-48c7-ac1f-1ac1d669a68a"
      },
      "source": [
        "outputs[\"no_columns\"] = {\n",
        "  \"mape\": mape_without_columns,\n",
        "  \"predicteds\": predictes_without_columns,\n",
        "  \"actuals\": actuals_without_columns,\n",
        "  \"dir_accs\": direction_accs_without_columns\n",
        "}\n",
        "\n",
        "pd.DataFrame(outputs).T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mape</th>\n",
              "      <th>predicteds</th>\n",
              "      <th>actuals</th>\n",
              "      <th>dir_accs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>1.64915</td>\n",
              "      <td>[[2998.4097, 3084.356, 3045.6523, 3120.0474, 3...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.6111111111111112, 0.7291666666666666, 0.694...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <td>1.67993</td>\n",
              "      <td>[[3005.8093, 3072.9053, 3052.4666, 3106.886, 3...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.625, 0.7222222222222222, 0.6944444444444444...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_anticipation</th>\n",
              "      <td>1.7529</td>\n",
              "      <td>[[3051.8484, 3051.8628, 3095.491, 3080.4194, 3...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.6388888888888888, 0.7291666666666666, 0.687...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_joy</th>\n",
              "      <td>1.74408</td>\n",
              "      <td>[[3000.5237, 3094.3665, 3048.2668, 3130.7449, ...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.5972222222222222, 0.7083333333333334, 0.666...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_trust</th>\n",
              "      <td>1.74323</td>\n",
              "      <td>[[3053.5486, 3037.826, 3096.5098, 3064.5942, 2...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.6319444444444444, 0.7083333333333334, 0.687...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_joy_anticipation</th>\n",
              "      <td>1.74537</td>\n",
              "      <td>[[3033.0828, 3056.1453, 3078.2205, 3086.1077, ...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.6319444444444444, 0.7083333333333334, 0.687...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_joy_trust</th>\n",
              "      <td>1.7125</td>\n",
              "      <td>[[2978.8682, 3093.0076, 3027.0288, 3131.2637, ...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.5972222222222222, 0.7083333333333334, 0.687...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear_joy_anticipation_trust</th>\n",
              "      <td>1.79462</td>\n",
              "      <td>[[3047.6711, 3039.7834, 3091.146, 3067.2085, 2...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.6319444444444444, 0.7291666666666666, 0.680...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>no_columns</th>\n",
              "      <td>1.70837</td>\n",
              "      <td>[[3022.9316, 3068.3594, 3068.3938, 3100.6118, ...</td>\n",
              "      <td>[[3003.37, 3130.12, 3023.94, 2972.37, 2746.56,...</td>\n",
              "      <td>[0.625, 0.7152777777777778, 0.6944444444444444...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                mape  ...                                           dir_accs\n",
              "fear                         1.64915  ...  [0.6111111111111112, 0.7291666666666666, 0.694...\n",
              "index                        1.67993  ...  [0.625, 0.7222222222222222, 0.6944444444444444...\n",
              "fear_anticipation             1.7529  ...  [0.6388888888888888, 0.7291666666666666, 0.687...\n",
              "fear_joy                     1.74408  ...  [0.5972222222222222, 0.7083333333333334, 0.666...\n",
              "fear_trust                   1.74323  ...  [0.6319444444444444, 0.7083333333333334, 0.687...\n",
              "fear_joy_anticipation        1.74537  ...  [0.6319444444444444, 0.7083333333333334, 0.687...\n",
              "fear_joy_trust                1.7125  ...  [0.5972222222222222, 0.7083333333333334, 0.687...\n",
              "fear_joy_anticipation_trust  1.79462  ...  [0.6319444444444444, 0.7291666666666666, 0.680...\n",
              "no_columns                   1.70837  ...  [0.625, 0.7152777777777778, 0.6944444444444444...\n",
              "\n",
              "[9 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8GYIxTmhLeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c69754b-9f37-47a3-d414-4a20a1d38027"
      },
      "source": [
        "dir_accs = pd.DataFrame(outputs).T[\"dir_accs\"]\n",
        "dir_accs.apply(np.mean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fear                           0.650000\n",
              "index                          0.636806\n",
              "fear_anticipation              0.647917\n",
              "fear_joy                       0.640278\n",
              "fear_trust                     0.635417\n",
              "fear_joy_anticipation          0.637500\n",
              "fear_joy_trust                 0.655556\n",
              "fear_joy_anticipation_trust    0.636111\n",
              "no_columns                     0.638194\n",
              "Name: dir_accs, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lzZgVgirOoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be376c7f-c31c-4c73-d47d-e0ae2e3903cb"
      },
      "source": [
        "best_lag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}